{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20d103e",
   "metadata": {},
   "source": [
    "Question 1:- Why is it generally preferable to use a logistic regression classifier\n",
    " rather than a classic perceptron (i.e., a single layer of threshold logic\n",
    " units trained using the perceptron training algorithm)? How can you\n",
    " tweak a perceptron to make it equivalent to a logistic regression\n",
    " classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25e49",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Classic Perceptron converge only if data is linearly separable but Logistic regression classifer perfrom well even if data is not linearly separable. Classic Perceptron also make hard classification and Logistic regression is able to estimate class probabilites.\n",
    "Replace the classic step funtion with sigmoid function or softmax if there are multiple neurons.\n",
    "Use the Gradient Descent to Minimize the Cross Entropy instead of Perceptron update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e01209",
   "metadata": {},
   "source": [
    "Why was the sigmoid activation function a key ingredient in training\n",
    " the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b383eb7",
   "metadata": {},
   "source": [
    "Before the Sigmoid function the problem with step/threshold functions was that.\n",
    "Step function was not differentiable, and it derivate was almost zero everywhere and Gradient based optimization was not possible.\n",
    "After the sigmoid function it derivative is non-zero and make Gradient based optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809074c",
   "metadata": {},
   "source": [
    "Name three popular activation functions. Can you draw them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5830d",
   "metadata": {},
   "source": [
    "1. ReLU (Rectified Linear Unit): Best for deep learning because it doesn't saturate on positive side and avoids vanishing gradient\n",
    "2. Sigmoid: Output layer of binary classification.\n",
    "3. Tanh: Hidden layers when data needs to be centered around zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af36007",
   "metadata": {},
   "source": [
    "6. Suppose you have an MLP composed of one input layer with 10\n",
    " passthrough neurons, followed by one hidden layer with 50 artificial\n",
    " neurons, and finally one output layer with 3 artificial neurons. All\n",
    " artificial neurons use the ReLU activation function.\n",
    "    1. What is the shape of the input matrix X?\n",
    "    2. What are the shapes of the hidden layer’s weight matrix W and\n",
    "    bias vector b ?\n",
    "    h\n",
    "    h\n",
    "    3. What are the shapes of the output layer’s weight matrix W and\n",
    "    bias vector b ?\n",
    "    o\n",
    "    o\n",
    "    4. What is the shape of the network’s output matrix Y?\n",
    "    5. Write the equation that computes the network’s output matrix Y\n",
    "    as a function of X, W , b , W , and b ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a5bf1",
   "metadata": {},
   "source": [
    "1. The shape of the input matrix X is m × 10, where m represents the training batch size.\n",
    "2. The shape of the hidden layer's weight matrix Wh is 10 × 50, and the length of its bias vector bh is 50.\n",
    "3. The shape of the output layer's weight matrix Wo is 50 × 3, and the length of its bias vector bo is 3.\n",
    "4. The shape of the network's output matrix Y is m × 3.\n",
    "5. Y = ReLU(ReLU(X Wh + bh) Wo + bo). Recall that the ReLU function just sets every negative number in the matrix to zero. Also note that when you are adding a bias vector to a matrix, it is added to every single row in the matrix, which is called broadcasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. How many neurons do you need in the output layer if you want to\n",
    " classify email into spam or ham? What activation function should you\n",
    " use in the output layer? If instead you want to tackle MNIST, how\n",
    " many neurons do you need in the output layer, and which activation\n",
    " function should you use? What about for getting your network to\n",
    " predict housing prices, as in Chapter 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e5a76",
   "metadata": {},
   "source": [
    "For the classify email only one output neuron and will sigmoid activation function.when estimating a probability.\n",
    "For MNIST ten output neurons and will use softmax function which can handle multiple classes, outputting one probability per class\n",
    "For predict Housing Prices only one neuron and there will any activation function. Because activation functions squash or transform values. For regression, we want the network to output any real number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Can you list all the hyperparameters you can tweak in a basic MLP? If\n",
    " the MLP overfits the training data, how could you tweak these\n",
    " hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a51200",
   "metadata": {},
   "source": [
    "1. To weak the basic MLP hyperparameters:\n",
    "    1. Hidden layers\n",
    "    2. Number of neuron in each hidden layer\n",
    "    3. Activation function in each layer\n",
    "    4. Activation function in output layer. For the output layer, in general you will want the sigmoid activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression.\n",
    "    5. If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
